---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(assertr)
library(readxl)
library(here)
library(janitor)

y2015_candy_data <- read_xlsx(here::here("raw_data/candy_ranking_data/boing-boing-candy-2015.xlsx"))

y2016_candy_data <- read_xlsx(here::here("raw_data/candy_ranking_data/boing-boing-candy-2016.xlsx"))

y2017_candy_data <- read_xlsx(here::here("raw_data/candy_ranking_data/boing-boing-candy-2017.xlsx"))

```

Woah, dat's a bit more data. Let's have a look. Also need to have a look at this message "New names:
* `` -> ...114"

```{r}
glimpse(y2015_candy_data)
```

```{r}
glimpse(y2016_candy_data)
```

```{r}
glimpse(y2017_candy_data)
```

Ok, so first of all, the message above refers to the fact that some columns have no names in the data so during the read in R has assigned some names for the columns. Looks like it refers specifically to column 114 so should have a look at this.

First thoughts:

1. Have a look at column 114 in each of the data sets.
2. We'll need to find a way to join each of the data sets. At first glance it looks like a couple of the datasets share a country column. Before deciding where to get cleaning first, we need to work out where the joins could occur.
3. Still not sure if the data is in the right format (i.e wide or long). There's a ton of columns but not sure how it would look if pivotted longer. First thoughts think it might not work. 
4. The column names are in a pretty bad way. I think we need to find a way to get everything in snake_case and also get columns with a similar names to have homogenized ones. 

Hints from notes:

1. You’ll need to combine these three datasets together.
2. The column country is particularly messy, you will likely need to do some ‘hard coding’ here!

After reading the extra info about the data:

1. It's mentioned that for the 2015 data there is no option for a "MEH" response.
2. There's loads of links to twitter and github with analysis published by other people. Could be really great for ideas is stuck. 

And after reading the questions: 

1. We're looking at a lot of totals (count) in opening questions. 
2. The data will probably need to be lengthened once we have all performed the join. So, hopefully, we can have years along the y axis of the table? Actually, now I think about it I'm not so sure. Let's just go for the fact that *something* will need to be done once we have a mammoth amount of columns. Or maybe before the join?
3. Need to really get a good idea of all the column names and where exactly we can get a join. Also, in line with the questioning, we should really think about what data we columns we can straight up get rid of. I'm sure that this is going to be beneficial, but I'm also a bit worried about getting rid of data that may have ended up in wrong column. Can do checks on this before deleting though. 


```{r}
names(y2015_candy_data)
```

```{r}
names(y2016_candy_data)
```

```{r}
names(y2017_candy_data)
```

After reviewing the notes, the following other ideas come to mind:

1. Should we just use janitor for this one in order to at least get the column names into better order?

2. We need to check on the amount of NA's, work out what type they are (hopefully not MNARs!) and then pick one of the following strategies:
  a) Drop the NAs
  b) Replace the NA values with something else
  c) Just leave them alone.
  
  No matter what the decision is, it should be justified.
  
3. Remember the three fundamental rules defining Tidy Data and then try to make sure your data abides by them:
  a) Each varible must have its own column
  b) Each observation should have its own row
  c) Each value must have it's own cell
  
  Allow these to guide you decisions on pivoting etc.
  

**Right let's have do a little more investigation and then see what we can come up with. 
Start with Na's 

```{r}
y2015_candy_data %>%
summarise(across(.fns = ~sum(is.na(.x))))
```

```{r}
y2016_candy_data %>%
summarise(across(.fns = ~sum(is.na(.x))))
```

```{r}
y2017_candy_data %>%
  summarise(across(.fns = ~sum(is.na(.x))))
```

Right, so we've got a significant amount of Na's, but I'm guessing since the first questsion tells us *not* to count any nas that means that we shouldn't replace them with anything but that we should keep them in by the time we get to that question. With this in mind, I'm deciding to leave the NAs in for now.

Think I'm going to go for cleaning the column names now. 

```{r}
cc_y2015_candy_data <-
  clean_names(y2015_candy_data)

cc_y2015_candy_data
```

```{r}
cc_y2016_candy_data <-
  clean_names(y2016_candy_data)

cc_y2016_candy_data
```

```{r}
cc_y2017_candy_data <-
  clean_names(y2017_candy_data)

cc_y2017_candy_data
```

Cool, that's looking a bit better now. Think that the next thing to do is to work out which columns can be dropped from all of the columns. The questions should definitely dictate this. I'm planning on doing this individually for each and then year before any join. Not sure if this is the right way to go about it but it's the way i'm feeling most comfortable with to be able to understand the process that is needed. Also will need to make sure that data hasn't gone into the wrong column before deleted columns. 

```{r}
cc_y2016_candy_data %>%
  distinct(which_country_do_you_live_in)

cc_y2016_candy_data %>%
  distinct(which_state_province_county_do_you_live_in)
```

```{r}
cc_y2017_candy_data %>%
  distinct(q4_country)

cc_y2017_candy_data %>%
  distinct(q5_state_province_county_etc)
```

Ok, it looks like we've not got any countries lingering in the county/state columns so I think we can add this to the list that we take out. This means, in my mind, that we can go ahead and take out the variables from each of the datasets that we don't need. Anything that is a candy or gives us some sort of information on age, gender or country is what we need to hold onto. Also anything that might allow us to fill in the blanks we have in terms of these variables.

```{r}
cc_y2015_candy_data %>%
  names()
```

```{r}
cc_y2016_candy_data %>% 
  names()
```

```{r}
cc_y2017_candy_data %>% 
  names
```

```{r}
cc_y2015_candy_data %>%
  distinct(please_leave_any_remarks_or_comments_regarding_your_choices)
```

Right, so looking at the 2015 data, we've obviously not got anything direct information for which country they are from. But we do have a few things that could indicate which country they are from. In particular, i'm thinking the type of smarties (american or commonwealth = USA or Canada?) and even the degrees of separation questions (low on j.k. rowling and thom yorke = UK?) Don't think the same can be done for gender. I think we'll go with these in terms of inclusion for now plus any other comments. 

```{r} 
# hasn't quite worked 
ds_group_count <- function(x){
  group_by(x) %>% 
  count(x)
  return(count(x))
  }
```

```{r}
# plan B
cc_y2015_candy_data %>%
  group_by(please_estimate_the_degrees_of_separation_you_have_from_the_following_folks_malala_yousafzai) %>% 
  count(please_estimate_the_degrees_of_separation_you_have_from_the_following_folks_malala_yousafzai)
```

```{r} 
#I know this might be a bit of a pain to drop columns by name, but if there is a problem of ordering or the like, I really don't want to lose the information we need. I've noticed that some of the degrees of separation questions duplicate celebrities, with some of the duplicates not containing values. I've made a wee chunk of code above to double check if there are values and if so how many. 

chosen_cols_2015 <-
cc_y2015_candy_data %>% 
  subset(
    select = -c(
      timestamp,
      please_leave_any_remarks_or_comments_regarding_your_choices:
        please_estimate_the_degree_s_of_separation_you_have_from_the_following_celebrities_francis_bacon_1561_1626,
    which_day_do_you_prefer_friday_or_sunday:please_estimate_the_degrees_of_separation_you_have_from_the_following_folks_beyonce_knowles)
  )

chosen_cols_2015

```

```{r}
cc_y2016_candy_data %>% 
  names()
```

```{r}
#Chosen columns to go in

chosen_cols_2016 <-
cc_y2016_candy_data %>%
  subset(
    select = -c(
      timestamp,
      which_state_province_county_do_you_live_in,
      please_list_any_items_not_included_above_that_give_you_joy:york_peppermint_patties_ignore)
  )

chosen_cols_2016
```

```{r}
cc_y2017_candy_data %>% 
  names
```

```{r}
# Columns chosen for 2017
chosen_cols_2017 <-
cc_y2017_candy_data %>%
  subset(
    select = -c(
      internal_id,
      q5_state_province_county_etc,
      q7_joy_other:click_coordinates_x_y)
  )
    
chosen_cols_2017
```


```{r}
cc_y2015_candy_data %>%
  select(please_estimate_the_degree_s_of_separation_you_have_from_the_following_celebrities_jk_rowling, please_estimate_the_degrees_of_separation_you_have_from_the_following_folks_jk_rowling)
```

```{r}
cc_y2015_candy_data %>%
  group_by(please_estimate_the_degrees_of_separation_you_have_from_the_following_folks_jk_rowling) %>% 
  count(please_estimate_the_degrees_of_separation_you_have_from_the_following_folks_jk_rowling)
```


When we are cleaning the ages, we should check if there are any numbers typed out for some reason. If not, we should just put in NAs for any values not in realistic range. 

Tomorrow, before the join, we rename the columns, or just match them up with those contained in other datasets. We should also think about sticking in an extra year/idea column for now which will allow us to make sure everthing is working.  


```{r}
full_join(chosen_cols_2015, chosen_cols_2016)
```

```{r}
chosen_cols_2015 %>% 
names()
```

```{r}
chosen_cols_2016 %>%
  names
```

```{r}
#Here, I added ids and years to each datasets. I also made sure that any observations that had no values in any variable were removed from the datasets. Looks like this only affected the 2017 data.

cc_id_2015 <-
chosen_cols_2015 %>%
  filter_all(any_vars(!is.na(.))) %>%
  mutate(year = 2015) %>%
  tibble::rowid_to_column("id")

cc_id_2016 <-
chosen_cols_2016 %>%
  filter_all(any_vars(!is.na(.))) %>% 
  mutate(year = 2016) %>%
  tibble::rowid_to_column("id")

cc_id_2017 <-
chosen_cols_2017 %>% 
  filter_all(any_vars(!is.na(.))) %>%
  mutate(year = 2017) %>%
  tibble::rowid_to_column("id")

cc_id_2015
cc_id_2016
cc_id_2017
```





```{r}
cc_pivot_2015 <-
cc_id_2015 %>%
  pivot_longer(
    cols = c(butterfinger:necco_wafers), 
    names_to = "candy_type", 
    values_to = "rating"
    )

cc_pivot_2015
```

```{r}
cc_pivot_2016 <-
cc_id_2016 %>%
  pivot_longer(
    cols = c(x100_grand_bar: york_peppermint_patties),
    names_to = "candy_type",
    values_to = "rating"  
    )

cc_pivot_2016
```

```{r}
cc_pivot_2017 <-
cc_id_2017 %>%
  pivot_longer(
    cols = c(q6_100_grand_bar: q6_york_peppermint_patties),
    names_to = "candy_type",
    values_to = "rating"
  ) %>%
  mutate(candy_type = str_remove(candy_type, fixed("q6_")))

cc_pivot_2017
```


```{r}
cc_pivot_2015_rename <-
  cc_pivot_2015 %>% 
  rename(
    age = how_old_are_you,
    going_out = are_you_going_actually_going_trick_or_treating_yourself,
  ) 

cc_pivot_2015_rename
```

```{r}
cc_pivot_2016_rename <-
  cc_pivot_2016 %>% 
  rename(
    going_out = are_you_going_actually_going_trick_or_treating_yourself,
    gender = your_gender,
    age = how_old_are_you,
    country = which_country_do_you_live_in
  )

cc_pivot_2016_rename
```

```{r}
cc_pivot_2017_rename <-
  cc_pivot_2017 %>% 
  rename(
    going_out = q1_going_out,
    gender = q2_gender,
    age = q3_age,
    country = q4_country
  )

cc_pivot_2017_rename
```

```{r}
joined_candy <-
bind_rows(cc_pivot_2015_rename, cc_pivot_2016_rename, cc_pivot_2017_rename)

joined_candy
```
Right, now that we've got the join done, let's go through each of the columns and make sure that everything is clean (enough!)

```{r}
joined_candy %>%
  distinct(age)
```

```{r}
#So by the look of it we want to retain all integer and convert all numbers with decimals to the closest integer. We also want to make sure that the ages are within a certain realistic age range. 

pars_exper <-
joined_candy %>% 
  mutate(age = parse_guess(age))
  
```

```{r}
pars_exper %>%
  distinct(age)
```

```{r}
integer_exper <-
joined_candy %>% 
  mutate(age = as.integer(age))

```

```{r}
integer_exper %>% 
  distinct(age)
```

```{r}
finish_exper <-
integer_exper %>%
  mutate(age = replace(age, age < 4 | age > 120, NA))
```

```{r}
#So by the look of it we want to retain all integer and convert all numbers with decimals to the closest integer. We also want to make sure that the ages are within a certain realistic age range. 

jc_age_done <-
joined_candy %>% 
  mutate(age = as.integer(age)) %>% 
  mutate(age = replace(age, age < 4 | age > 120, NA))

jc_age_done %>% 
distinct(age)
```

```{r}
jc_age_done %>%
  group_by(going_out) %>% 
  summarise(count = n()
            )
```

```{r} 
#Right, so it looks like there are a couple of candies that are repeated in different forms. I'm going to keep these the way they are right now and then maybe provide alternativer answers in the analysis i.e. if we consider "anonymous_brown_globs" as mary_janes as suggested then maybe we can amalgamate these scores. We would need to find a way of making sure that doublecounting wasn't involved. ie. id 3 from 2015 cannot register two votes for both names of mary janes. 

jc_age_done %>%
  distinct(candy_type) %>%
  arrange(candy_type)
```

```{r}
#This is the biggie, let's see what we can do with the countries
```

```{r}
jc_age_done %>%
  distinct(country)
```

```{r}
num_for_country <-
jc_age_done %>% 
  mutate(country_num_detect = str_detect(country, "[0-9]+[0-9]"))

num_for_country
```

```{r}
num_changed_for_country <-
num_for_country %>%
  mutate(age = if_else(country_num_detect == TRUE, country, age))

num_changed_for_country
```

```{r}
num_changed_for_country %>% 
  distinct(age)

```

```{r}
#Given up on this, just going to hard code the misplaced ages in. 
num_for_country_hard_code <-
jc_age_done %>% 
  mutate(country_num_detect = str_detect(country, "[0-9]+[0-9]")) %>% 
  filter(country_num_detect == TRUE) %>% 
  select(id, year, country)

num_for_country_hard_code
```

```{r}
#Hardcoding the replacement ages
jc_age_extras <-
jc_age_done %>% 
  mutate(
    age = replace(age, id == 117 & year == 2016, 51),
    age = replace(age, id == 303 & year == 2016, 47),
    age = replace(age, id == 622 & year == 2016, 54),
    age = replace(age, id == 623 & year == 2016, 54),
    age = replace(age, id == 692 & year == 2016, 44),
    age = replace(age, id == 829 & year == 2016, 45),
    age = replace(age, id == 1101 & year == 2016, 30),
    age = replace(age, id == 186 & year == 2017, 35),
    age = replace(age, id == 558 & year == 2017, 46),
    age = replace(age, id == 728 & year == 2017, 45),
  )

jc_age_extras
```

```{r}
jc_age_extras %>%
  distinct(country)
```

```{r}
#Before getting recoding the countries, I want to get rid of the numeric and nonsense values

jc_age_extras %>%
  mutate(
    country = replace(country,country == "[Uu]+[Ss]+[Aa]", "USA")
  )
```

