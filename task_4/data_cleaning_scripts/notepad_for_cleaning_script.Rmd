---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(assertr)
library(readxl)
library(here)
library(janitor)

y2015_candy_data <- read_xlsx(here::here("raw_data/candy_ranking_data/boing-boing-candy-2015.xlsx"))

y2016_candy_data <- read_xlsx(here::here("raw_data/candy_ranking_data/boing-boing-candy-2016.xlsx"))

y2017_candy_data <- read_xlsx(here::here("raw_data/candy_ranking_data/boing-boing-candy-2017.xlsx"))

```

Woah, dat's a bit more data. Let's have a look. Also need to have a look at this message "New names:
* `` -> ...114"

```{r}
view(y2015_candy_data)
```

```{r}
view(y2016_candy_data)
```

```{r}
view(y2017_candy_data)
```

```{r}
glimpse(y2015_candy_data)
```

```{r}
glimpse(y2016_candy_data)
```

```{r}
glimpse(y2017_candy_data)
```

Ok, so first of all, the message above refers to the fact that some columns have no names in the data so during the read in R has assigned some names for the columns. Looks like it refers specifically to column 114 so should have a look at this.

First thoughts:

1. Have a look at column 114 in each of the data sets.
2. We'll need to find a way to join each of the data sets. At first glance it looks like a couple of the datasets share a country column. Before deciding where to get cleaning first, we need to work out where the joins could occur.
3. Still not sure if the data is in the right format (i.e wide or long). There's a ton of columns but not sure how it would look if pivotted longer. First thoughts think it might not work. 
4. The column names are in a pretty bad way. I think we need to find a way to get everything in snake_case and also get columns with a similar names to have homogenized ones. 

Hints from notes:

1. You’ll need to combine these three datasets together.
2. The column country is particularly messy, you will likely need to do some ‘hard coding’ here!

After reading the extra info about the data:

1. It's mentioned that for the 2015 data there is no option for a "MEH" response.
2. There's loads of links to twitter and github with analysis published by other people. Could be really great for ideas is stuck. 

And after reading the questions: 

1. We're looking at a lot of totals (count) in opening questions. 
2. The data will probably need to be lengthened once we have all performed the join. So, hopefully, we can have years along the y axis of the table? Actually, now I think about it I'm not so sure. Let's just go for the fact that *something* will need to be done once we have a mammoth amount of columns. Or maybe before the join?
3. Need to really get a good idea of all the column names and where exactly we can get a join. Also, in line with the questioning, we should really think about what data we columns we can straight up get rid of. I'm sure that this is going to be beneficial, but I'm also a bit worried about getting rid of data that may have ended up in wrong column. Can do checks on this before deleting though. 


```{r}
names(y2015_candy_data)
```

```{r}
names(y2016_candy_data)
```

```{r}
names(y2017_candy_data)
```

After reviewing the notes, the following other ideas come to mind:

1. Should we just use janitor for this one in order to at least get the column names into better order?

2. We need to check on the amount of NA's, work out what type they are (hopefully not MNARs!) and then pick one of the following strategies:
  a) Drop the NAs
  b) Replace the NA values with something else
  c) Just leave them alone.
  
  No matter what the decision is, it should be justified.
  
3. Remember the three fundamental rules defining Tidy Data and then try to make sure your data abides by them:
  a) Each varible must have its own column
  b) Each observation should have its own row
  c) Each value must have it's own cell
  
  Allow these to guide you decisions on pivoting etc.
  

**Right let's have do a little more investigation and then see what we can come up with. 
Start with Na's 

```{r}
y2015_candy_data %>%
summarise(across(.fns = ~sum(is.na(.x))))
```

```{r}
y2016_candy_data %>%
summarise(across(.fns = ~sum(is.na(.x))))
```

```{r}
y2017_candy_data %>%
  summarise(across(.fns = ~sum(is.na(.x))))
```

Right, so we've got a significant amount of Na's, but I'm guessing since the first questsion tells us *not* to count any nas that means that we shouldn't replace them with anything but that we should keep them in by the time we get to that question. With this in mind, I'm deciding to leave the NAs in for now.

Think I'm going to go for cleaning the column names now. 

```{r}
cc_y2015_candy_data <-
  clean_names(y2015_candy_data)

cc_y2015_candy_data
```

```{r}
cc_y2016_candy_data <-
  clean_names(y2016_candy_data)

cc_y2016_candy_data
```

```{r}
cc_y2017_candy_data <-
  clean_names(y2017_candy_data)

cc_y2017_candy_data
```

Cool, that's looking a bit better now. Think that the next thing to do is to work out which columns can be dropped from all of the columns. The questions should definitely dictate this. I'm planning on doing this individually for each and then year before any join. Not sure if this is the right way to go about it but it's the way i'm feeling most comfortable with to be able to understand the process that is needed. Also will need to make sure that data hasn't gone into the wrong column before deleted columns. 

```{r}
cc_y2016_candy_data %>%
  distinct(which_country_do_you_live_in)

cc_y2016_candy_data %>%
  distinct(which_state_province_county_do_you_live_in)
```

```{r}
cc_y2017_candy_data %>%
  distinct(q4_country)

cc_y2017_candy_data %>%
  distinct(q5_state_province_county_etc)
```

Ok, it looks like we've not got any countries lingering in the county/state columns so I think we can add this to the list that we take out. This means, in my mind, that we can go ahead and take out the variables from each of the datasets that we don't need. Anything that is a candy or gives us some sort of information on age, gender or country is what we need to hold onto. Also anything that might allow us to fill in the blanks we have in terms of these variables.

```{r}
cc_y2015_candy_data %>%
  names()
```

```{r}
cc_y2016_candy_data %>% 
  names()
```

```{r}
cc_y2017_candy_data %>% 
  names
```

Right, so looking at the 2015 data, we've obviously not got anything direct information for which country they are from. But we do have a few things that could indicate which country they are from. In particular, i'm thinking the type of smarties (american or commonwealth = USA or Canada?) and even the degrees of separation questions (low on j.k. rowling and thom yorke = UK?) Don't think the same can be done for gender. I think we'll go with these in terms of inclusion for now. 

```{r}
chosen_2015_candy_data <- 
cc_y2015_candy_data
```

